\documentclass[14pt,a4paper]{extarticle}
%\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[ukrainian]{babel}


\usepackage{amssymb}
\usepackage{physics}


\usepackage[active]{srcltx}
\usepackage[final]{pdfpages}

\usepackage[hidelinks]{hyperref}

\usepackage{verbatim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\pagestyle{empty}                     %нумерацiя сторiнок i т.д.
\pagestyle{headings}                   %нумерацiя сторiнок вгорi зправа i т.д.
%\renewcommand{\baselinestretch}{1.5}   %мiжстрiчковий інтервал
%\parindent=7.5mm                      %абзацний відступ
 \righthyphenmin=2                     %перенос 2 останніх букв
 \pagenumbering{arabic}
 \tolerance=400
 \mathsurround=2pt
 \hfuzz=1.5pt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \hoffset=-0.5cm        %+2.5cm -- вiдступ вiд лiвого краю
 \voffset=-1.5cm        %+2.5cm -- вiдступ зверху
 \oddsidemargin=0.1cm   %ліве поле
 \topmargin=0.1cm       %верхнє поле
 \headheight=0.5cm      %висота верхнього колонтитулу
 \footskip=1cm          %висота нижнього колонтитулу
 \headsep=0.3cm         %відступ від колонт. до тексту
 \textwidth=17cm        %ширина сторінки
 \textheight=25.5cm     %висота сторінки
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newcounter{e}
 \setcounter{e}{0}
 \newcommand{\n}{\refstepcounter{e} (\arabic{e})}
 
 \newcounter{pic}
 \setcounter{pic}{0}
 \newcommand{\pic}[1]{\refstepcounter{pic} \vspace{-0.3cm}\textit{Рисунок \arabic{pic}\label{#1}.}}
 
 \newcounter{tabl}
 \setcounter{tabl}{0}
 \newcommand{\tabl}[1]{\refstepcounter{tabl} \vspace{-0.3cm}\textit{Таблиця \arabic{tabl}\label{#1}.}}
 
 \newcounter{dod}
 \setcounter{dod}{0}
 \newcommand{\dod}[1]{\refstepcounter{dod} \textit{Додаток \arabic{dod}\label{#1}.}}
 
% \newcounter{defn}
 %\setcounter{defn}{0}
 %\newcommand{\defn}[1]{\refstepcounter{defn} %\textbf{Означення \arabic{defn}\label{#1}.}}
 
 %\newcounter{theorem}
 %\setcounter{theorem}{0}
 %\newcommand{\theorem}[1]{\refstepcounter{theorem} %\textbf{Теорема \arabic{theorem}\label{#1}.}}
 \newtheorem{theorem}{Теорема}[section]
 \newtheorem{defn}[theorem]{Означення}
 \newtheorem{lemma}[theorem]{Лема}
 
 \newcommand{\proof}{\textit{Доведення. \space}}
% \setcounter{page}{1}
% \setcounter{section}{1}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newcounter{stali}
 \setcounter{stali}{0}
 \newcommand{\s}{\refstepcounter{stali} \arabic{stali}}

 \newcommand{\st}{C_{\s}}
 \newcommand{\stl}[1]{C_{\s \label{#1}}}

 \newcommand{\cd}{{} $$ \vspace{-0.3cm} $$ {}}
 
 \newcommand{\nb}[2]{\righthyphenmin=#2 #1 \righthyphenmin=2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 \newcommand{\tabboxl}[2]{\parbox{#1}{\vspace{0.1cm} #2 \vspace{0.1cm} }}
 
 
 \newcommand{\tabboxr}[2]{\parbox{#1}{\vspace{-0.3cm}
 		\begin{flushright} #2 \end{flushright} \vspace{-0.3cm} }}
 
 \newcommand{\tabboxc}[2]{\parbox{#1}{\vspace{-0.3cm}
 		\begin{center} #2 \end{center} \vspace{-0.3cm} }}

 \newcommand{\liml}{\lim\limits}
 \newcommand{\suml}{\sum\limits}
 \newcommand{\intl}{\int\limits}
 
 \newcommand{\inttwopi}{\intl_{0}^{2\pi}}
 
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % bibliography
 %\usepackage[
 %backend=biber,
 %style=numeric,
 %sorting=none
 %]{biblatex}
 %\addbibresource{resources/bibliography.bibtex}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \begin{document}
	
 %\bibliographystyle{insrt}

 \thispagestyle{empty}

 \begin{center}
	\large
	Міністерство освіти і науки, молоді та спорту України \\
	Львівський національний університет імені Івана Франка \\
	Факультет прикладної математики та інформатики \\
	Кафедра обчислювальної математики
 \end{center}

 \vspace{45pt}

 \vfill

 \begin{center}
	{\Huge{Курсова робота}}\\
	{\large на тему:}
 \end{center}

 \begin{center}\Large
	\textbf{\emph{"Розробка алгоритмів захисту від атак на глибокі нейронні мережі"}}
 \end{center}

 \vfill
 \vskip100pt

 \begin{flushleft}
	\hskip8cm 
	Виконав:
	\\ \hskip8cm 
	студент IV курсу групи ПМп-41
	\\ \hskip8cm
	напрямку підготовки (спеціальності)
	\\ \hskip8cm
	113 -- ``Прикладна математика''
	\\ \hskip8cm
	Бугрій Б.О.
 \end{flushleft}

 \begin{flushleft}
	\hskip8cm 
	Науковий керівник:
	\\ \hskip8cm
	доц. Музичук Ю.М.
 \end{flushleft}

 \vfill

 \begin{center}
	\large
	Львів - 2021
 \end{center}

 \newpage
 \thispagestyle{empty}
 \tableofcontents

 \newpage
 \thispagestyle{empty}
 \addcontentsline{toc}{section}{Вступ}
 \section*{Вступ}
 \begin{center}\end{center}

 \begin{comment}
В останні десятиліття людству, завдяки використанню розумних систем штучного інтелекту, вдалося досягти вражаючих результатів у багатьох сферах повсякденного життя. Яскравими прикладами цього є автомобілі, які здатні долати складні маршрути без жодного людського втручання, програмне забезпечення, яке ставить пацієнтам точні діагнози на основі аналізів та іншої детальної інформації, застосунки для розпізнавання та відтворення мовлення і багато іншого.

Значна частина цих проривів завдячує стрімкому розвитку глибоких нейронних мереж (Deep Neural Networks, DNN), які, в порівнянні з іншими алгоритмами машинного навчання, здатні показувати неймовірні результати на основі великих об'ємів даних, іноді навіть перевершуючи людей. 

Такі типи алгоритмів машинного навчання часто використовують антиінтуїтивні, в порівнянні з смисловим значенням, закономірності стосовно певних характеристик \cite{first-work}. Через це вони є вразливими до різного роду зловмисних модифікацій вхідних даних, які можуть призвести до невірних передбачень. Тому ми не можемо бути впевнені в результатах передбачень моделей на 100\%.
\end{comment}

Сьогодні розумні системи штучного інтелекту є невід'ємною частиною життєдіяльності суспільства. Завдяки таким системам людству вдалося досягти значних результатів у багатьох сферах та галузях, таких як, наприклад, медицина, програмна інженерія, машинобудування та робототехніка.

Зараз найбільш поширеним типом алгоритмів машинного навчання є глибокі нейронні мережі, які здатні знаходити закономірності у великих масивах даних, а результати їх роботи часто перевершують людей. Проте, як показали нещодавні дослідження \cite{first-work}, такі алгоритми часто використовують антиінтуїтивні, в порівнянні з смисловим значенням, закономірності стосовно певних характеристик. Через це нейронні мережі є вразливими до різного роду зловмисних втручань, які можуть призвести до невірних результатів.

Моделі, які показують відмінні результати на звичайних даних, можуть бути легко ошуканими зразками, які лише трохи відрізняються від правильно класифікованих прикладів. Це розкриває фундаментальні недоліки в алгоритмах машинного навчання, які можуть бути використані зловмисниками для заподіяння шкоди, що може вплинути на безпеку технологічних процесів людської життєдіяльності і призвести до катастроф великого маштабу.

%-------------------------------------------------------------------------------
Природнім є бажання розробити архітектури та алгоритми тренування нейронних мереж, які зменшують ризики таких атак та є стійкими до зловмисних збурень. Постає питання, які саме фактори впливають на стійкість моделі, що робіть її більш вразливою до атак, а що ні. 

... TODO


???

Щоб ефективно захиститись від можливих загроз, потрібно знати слабкі місця алгоритмів та стратегії нападу, що використовують зловмисники. Саме тому ми розглянемо деякі підходи до атак на нейронні мережі, знання яких дозволить покращити системи машинного навчання, зробити їх більш стійким до зловмисних втручань. Також необхідним буде ввести метрикі стійкості мереж, щоб мати можливість об'єктивно порівняти отримані результати та обрати оптимальні з них. 



У цій роботі ми розглянемо два 
абсолютно різні
 підходи до побудови захисту проти ошукуючих атак та спробуємо поєднати їх для отримання бажаного результату.

... TODO






 
 \newpage
 \thispagestyle{empty}
 \section{Опис проблеми}
 
 Для повного розуміння поставленої мети, у цьому розділі ми формалізуємо постановку задачі побудови нейронних мереж, стійких до ошукуючих атак. Також ми опишемо пов'язані терміни та поняття, типи захистів та іншу інформацію, яку потім використаємо в наступних розділах.
 
 \subsection{Постановка задачі}
 %Зменшення множини ошукуючих зразків
 
 Нехай система машинного навчання $M$ на основі зразків $x \in S$ робить передбачення $y$. Тут $S$ -- множина всеможливих зразків-зображень з предметної області, які допустимі для використання моделлю $M$.
 
 \begin{defn}
 	Зразок $x^{adv} = x + \tau$, $x^{adv} \in S$ називається ошукуючим якщо для достатньо малих збурень $\tau$ виконуються такі умови:  
 	\begin{equation}
	 	M(x) = y_{true}
	\end{equation}
	\begin{equation}
	 	M(x^{adv}) \neq y_{true}
 	\end{equation}
 	де $y_{true}$ - правильне передбачення. 
 \end{defn}
 % норма тау менша за деяку константу
 
 Нашою метою є побудова максимально ефектвної моделі машинного навчання, яка буде менш вразливою до такого типу ошукуючих зразків. Задачу захисту моделі від ошукуючої атаки можна формалізувати наступним чином.
 
 Нехай $S^{adv}(M) \subset S$ -- множина ошукуючих зразків для моделі $M$. Необхідно знайти модель $M'$ яка є, в певному сенсі, модифікацією оригінальної моделі, таку, що
 \begin{equation}
 	S^{adv}(M') = \emptyset.
 \end{equation}
 Мається на увазі, що для моделі $M'$ неможливо створити ошукуючі зразки, тобто вона є невразливою до атак. 

 Такий ``ідеальний'' випадок є практично неможливим, тому ми будемо використовувати пом'якшене формулювання, а саме вимагатимемо, щоб для моделі-образа задовільнялася умова
 \begin{equation}
 	n(S^{adv}(M')) < n(S^{adv}(M))
 \end{equation}
 де $n(S)$ --кількість елементів в множині S. Іншими словами, нашою метою є побудова моделі, більш стійкої за оригінал.
 
 \subsection{Альтернативна постановка задачі}
 %Невеликі зміни зразків не призводять до значних змін результатів
 Модель машинного навчання вважається стійкою до ошукуючих зразків якщо результат передбачення не зміниться при невеликих збуреннях ...
 \subsection{Пов'язані поняття}
 У своїй минулій роботі ...
 
 Теорія, типи захисту, означення ...
 
 
  - Типи захисту (основні поняття і означення)
  
 > за типом атак (від чого захищатися (від ошукуючих атак, від викрадення і отруєння т.д.))
 
 > за природою захисту (з модифікацією моделі, з модифікацією оболонки (середовища), з модифікацією тренувального набору ...)
 
 *додати посилання
 
 
 У даній роботі ми розглянемо такі то типи
 
 
 \newpage
 \thispagestyle{empty}
 \section{Глибокі нейронні мережі}
 
 \newpage
 \thispagestyle{empty}
 \section{Визначення стійкості}
 
 Під час проведення досліджень важливою є можливість порівняти отримані результати щоб визначити ефективність того чи іншого методу та вибрати підхід до тренування та застосування моделі, який робить її більш стійкою до ошукуючих зразків.
 
 Отже, потрібно якимсь чином виміряти стійкість моделі. Тут варто згадати, якими метриками користуються зловмисники. У нашій попередній роботі \cite{my-work} ми вже згадували, що під час атак основною метою є знайти зразок $x^{adv}$ який є найближчим до оригінального зразка $x$.
 Щоб визначити якість ошукуючих зразків зловмисники часто використовують норму  $L_p$, де $1 \leq p<\infty$. Вона має вигляд
 
 \begin{equation}
 \label{ln-pretrub}
 \|x-\tilde{x}\|_{p}=\left(\sum_{i=1}^{n}\left(x_{i}-\tilde{x}_{i}\right)^{p}\right)^{\frac{1}{n}}
 \end{equation}
 
 
 
 
 Природнім є обмежувати розмір шуму, який зловмисники накладають на оригінальні зображення. Ці обмеження також часто є залежними від вищезгаданої норми.
 
 Значення \ref{ln-pretrub} буде дуже малим, якщо шум складається з невеликих змін для багатьох пікселів, що, загалом, свідчить про те що людині складно буде відрізнити ошукуючий зразок від оригнального. З цього припущення випливає, що якщо шум $\tau$ є достатньо великим, то ошукуючий зразок було важче створити і його легше можна виявити.
 
 Ба більше, як ми вже згадували в нашій попередній роботі, деякі атаки ...
 


 for $p=\infty$, it is $\|\alpha\|_{\infty}=\max _{i}\left|\alpha_{i}\right|$. Also commonly used is
 the 0 -norm (which is technically not a norm): $\|\alpha\|_{0}=\mid\{i:$ $\left.\alpha_{i} \neq 0\right\} \mid .$ A small 0-norm attack is permitted to arbitrarily change a few entries of the input; for example, an attack on the image recognition system for self-driving cars based on putting a sticker in the field of vision is such an attack [19].
 
 
 Small $p$ -norm attacks for larger values of $p$ (including $p=\infty$ ) require the changes to the pixels to be small in an aggregate sense, but the changes may be spread out over many or all features.
 
 
 A change in the lighting condition of an image may correspond to such an attack [34], [50]. The latter attacks are generally considered more powerful, as they can easily remain invisible to human observers. Other attacks that are not amenable to norm bounding exist [67], [54], [66], but this paper deals exclusively with norm-bounded attacks.
 
 
 Нехай $B_{p}(r):=\left\{\alpha \in \mathbb{R}^{n}:\|\alpha\|_{p} \leq r\right\}$ це куля, утворена на основі $p$ -норми
 з радіусом $r$. Для заданої моделі класифікації, $M$, та фіксованого вхідного зразка, $x \in \mathbb{R}^{n}$, зловмисники можуть успішно створити ошукуючий зразок $x^{adv}$ з розміром шуму $L$ для обраної $p$ -норми якщо їм вдастся знайти $\tau \in B_{p}(L)$ таке, що $M(x+\tau) \neq M(x)$. Узагальнивши, можна сказати що атакуючі намагаються знайти якнайменше $\tau$ яке змінить результат класифікації.
 
 Інтуітивно зрозуміло, що модель можна розглядати як стійку, в певному сенсі,
 до ошукуючих зразків, якщо результати її передбачень є нечутливими до малих змін будь-якого прийнятного вхідного зразка.
 
 Важко формально визначити, які зразки можна вважати прийнятними. Так як перевірка продуктивності моделі проводиться на тестувальних даних які не використовують під час тренування моделі, то такий набір можна застосувати як і початкову точку для оцінки стійкості.
 
 \begin{defn}
 Модель $M$ називається стійкою до атак за нормою $L_p$ на заданому зразку $x$ коли $M(x)=M(x+\tau)$ $\forall \tau \in B_{p}(L) .$ Якщо $M$ це НМ для багатокласової класифікації, то це рівняння еквівалентне до:
 $$
 \forall \tau \in B_{p}(L): y_{k}(x+\tau)>\max _{i: i \neq k} y_{i}(x+\tau)
 $$
 де $k:=M(x)$.
 \end{defn}
 [TODO] переглянути позначення.
 
 Тобто якщо модель є певною мірою стійкою то малі зміни вхідного зразка не змінюють оцінки настільки, щоб змінити результат -- мітку-передбачення. Можемо узагальнити даний принцип на весь тренувальний набір для визначення стійкості моделі використавши середню відстань від оригінальних зразків до ошукуючих .
 
 \begin{defn}
 Нехай $M$ -- НМ для багатокласової класифікації, $x \in X_{test}$ -- тестувальний набір, $x^{adv} \in S^{adv}(M, X_{test})$ -- ошукуючі зразки, які відповідають прикладам з тестового набору. Тоді стійкість моделі на обраному наборі даних та з використанням відповідної норми визначимо як
 $$
 r_p(M, X_{test}) := \frac{\sum\|x-x^{adv}\|_{p}}{n(X_{test})}
 $$
 
 [TODO] Запису суми ... n = $n(X_{test})$
 
 [TODO] $X_{avd} := S^{adv}(M, X_{test})$
 
 [TODO] $r_p(M) \approx r_p(M, X_{test}), n(X_{test}) \to \infty$
 
 \end{defn}
 TODO...
 
 Метрику $r_p(M, X_{test})$ зручно використовувати ...
 
 В окремих випадках залежність кількості від ітерацій ...
 
 
 \newpage
 \thispagestyle{empty}
 \section{Огляд атак на моделі машинного навчання}

 -- TODO
 
 -- Вступ до розділу, посилання на термінологію з курсової
 
 -- Типи атак, чорна та біла скриньки


 Нейронні мережі є особливо чутливими до атак на білу скриньку. Отримавши доступ до всіх параметрів, у зловмисників з'являється можливість обчислити точні похідні для функцій (у випадку диференційовності моделі), за допомогою яких виконується класифікація. Тоді під час атаки можна застосувати різні чисельні методи для вирішення конкретної проблеми, що дозволяє досягти дуже високої ефективності згенерованих ошукуючих зразків.
 % Окрім того, складно створити дієві механізми захисту моделі від таких атак, бо зловмисник може врахувати їх при створенні ошукуючих зразків. 

 \begin{center}
	\includegraphics[width=10cm]{../images/six.png}
 \end{center}
 \begin{center}
	\pic{six}
	Початкове зображення класифікується нейронною мережею як ``шість''. Після додавання до нього певного шуму НМ класифікує його як ``п'ять''.
 \end{center}


 Метод швидкого градієнтного спуску є одним з найпростіших методів генерації ошукуючих прикладів. Його застосовують для атак на різні системи машинного навчання, зокрема й на нейронні мережі \cite{explaining-a-e}.

 Основною метою методу є максимізація функції витрат $J$, розглядаючи її як $J(x, y_{true})$, зафіксувавши ваги нейронної мережі. В такому випадку потрібно знайти $x^{adv}$ таке, що виконується нерівність: 

 $$
 J\left(x^{adv}, y_{true}\right)>J\left(x, y_{true}\right)
 \eqno \n \label{convergency}
 $$
 Тоді, згідно з \cite{quarteroni}, ми зможемо знайти розв'язок поставленої задачі. Пошук ошукуючих зразків здійснюємо за формулою:
 $$
 x^{adv} = x + \epsilon {sign}\left(\nabla_x J\left(x, y_{true}\right)\right)
 \eqno \n \label{FGSM}
 $$
 де $\epsilon$ - додатній параметр, який називатимемо \textit{розміром кроку}. Напрямок ${sign}\left(\nabla_x J\left(x, y_{true}\right)\right)$ є \textit{напрямком зростання} для функції $J$.

 Варто зауважити, що метод не гарантує знаходження ошукуючого зразка, бо нерівність (\ref{convergency}) свідчить лише про збільшення функції правдоподібності. Тому в загальному випадку доцільно використовувати ітераційну форму методу. Також можна застосовувати інші методи, такі як \cite{C-and-W}, для того, щоб зменшити розмір шуму $\tau$.

 Більшість реальних систем, які використовують нейронні мережі, не розголошують своїх конфігурацій (як от структура мережі і її ваги), тому атаки, накшталт швидкого градієнтного спуску не так просто застосувати на практиці. В деяких сценаріях зловмисники мають можливість використовувати модель для отримання передбачень на основі власних зразків (як от, наприклад, при використанні  програмних інтерфейсів від Google AI чи Amazon). Запити до мережі  дозволено повторювати необмежену кількість разів і на основі результатів покращувати ошукуючі зразки.

 Тоді є можливість обчислити градієнт наближено, для чого достатньо лише результату класифікації $f(x)$ та відповідного вхідного параметра $x$, як показано в \cite{zoo}. Якщо обчислимо часткові похідні наближено, за формулою

 $$
 \frac{\partial J(\mathbf{x})}{\partial \mathbf{x}_{i}} \approx
 \lim_{h \to 0} \frac{J\left(\mathbf{x}+h e_{i}\right)-J\left(\mathbf{x}-h e_{i}\right)}{2h},
 \eqno \n \label{df-zoo}
 $$
 де $e_i$ - вектор, в якому $i$-тий елемент дорівнює одиниці, а всі решта -- нулю, то зможемо застосувати для атаки на чорну скриньку алгоритми, що використовують похідні для розв'язання задачі оптимізації, такі як швидкий градієнтний спуск.

 При використанні наближених значень похідних в методі швидкого градієнту (\ref{FGSM}) вдається досягти результативності, близької до тої, яку отримуємо під час атак на білу скриньку. Це зумовлено тим, що для ефективної роботи методів достатньо лише знаку градієнту. Те, що значення обчислені з певною похибкою, значним чином не впливає на результат. Недоліком такого сценарію є необхідність виконувати класифікацію значну кількість разів, що займає багато часу та ресурсів і може бути легко виявлено.

 TODO розписати детальніше, можливо поділити на розділи


 \subsection{Швидкий градієнтний спуск}
 \subsection{Метод Карліні і Вагнера}
 \subsection{Transferability}
 
 \newpage
 \thispagestyle{empty}
 \section{Принципи захисту від ошукуючих атак}
 
 Як бачимо з попереднього розділу, для атак, які базуються на створенні ошукуючих зразків, характерним є використання градієнтів, обчислених для оцінки чутливості мережі до зміни вхідних даних. Такі градієнти також називають ошукуючими \cite{defencive-distillation}. Очевидно, що при великих значеннях ошукуючих градієнтів зловмиснику значно простіше знайти відповідний ошукуючий зразок для деякого вхідного зображення, бо невеликі збурення вхідного зразка призведуть до значної зміни результату передбачення.
 
 Таким чином, щоб збільшити стійкість мережі до ошукуючих атак, необхідно зменшити таку різку залежність від вхідних даних, а отже і амплітутду коливань ошукуючих градієнтів. Це означає що нам потрібно ``згладити'' модель, отриману під час стандартного процесу навчання, щоб допомогти їй краще узагальнювати зразки, які не є частиною тренувального набору.
 
 Варто звернути увагу на те, що ошукуючі зразки не обов'язково мають бути створені штучно на основі дослідження мережі-цілі, спеціально для того щоб отримати бажаний результат класифікації. У цьому розділі ми побачимо, що в ``природі'' таких прикладів також вдосталь, причому деякі неправильно натреновані нейронні мережі є до них особливо вразливими. 
 
 В попередніх розділах [TODO] ми визначили надійність глибоких нейронних мереж як їх стійкість до обмежених збурень вхідних даних. Іншими словами, надійна модель повинна показувати високу точність не лише всередині навчального набору даних, а й за його межами, тобто моделювати функцію, яка ``інтуїтивно'' розподіляє віхідні дані між запропонованими категоріями в околі даного зразка. Цей окіл може бути визначений нормою у відповідному просторі.

 [TODO]
 \cite{analysis-of-robustness}
 Надійність використання норми при визначенні стійкості до атак 

 \begin{center}
 	\includegraphics[width=10cm]{../images/empty.pdf}
 \end{center}
 \begin{center}
 	\pic{domain} 2D ілюстрація.
 \end{center}
 
 \begin{comment}
 [TODO]
 
 Отже, ошукуючі зразки не обов'язково витягуються з вхідного розподілу який ми намагаємось змоделювати під час навчання, використовуючи архітектуру DNN.
 
 \end{comment}
 
 
 
 \subsection{Боротьба з штучно створеним шумом ? природа ошукуючих зразків}
 Не просто  знайти ефективний підхід до боротьби з ошукуючими збуреннями, при цьому не завдавши шкоди точність моделі.
 
 Adversarial training
 
 Example detection
 
 Generalization (зменшення розмірності)
 
 
 Додавання випадкового шуму
 
  
 \subsection{Надмірне тренування як причина вразливості}
 
 Працюючи ...
 На думку спадає ще один феномен у машинному навчанні, який має схожу причину -- \textit{перетренування (overfitting)}.
 
 І справді, бачимо залежність між стійкістю моделі та надмірним тренуванням. Перетреновані моделі надзвичайно вразливі до штучно створених ошукуючих зразків. Більше того, нівіть класифікація зразків з тестувального набору є для них складною задачею.
 
 Така поведінка зумовлена тим, що границя рішень .....
 
 
 
 \begin{LARGE}
 	DNN vs CNN
 \end{LARGE}
 
 \newpage
 \thispagestyle{empty}
 
 
 \newpage
 \thispagestyle{empty}
 \section{Захисна дистиляція та її модифікації}
 \subsection{Дистиляція нейронних мереж}
 
 В цьому розділі ми розглянемо підхід до оптимізації нейронної мережі або групи нейронних мереж, метою якого є зменшення обчислювальні затрат та ресурсів під час функціонування моделі. Це підхід був розроблений інженерами компанії Google Хілтоном на ін. і описаний в \cite{distillation}. Не зважаючи на те, що першочерговою метою дистиляції є зменшення розміру нейронної мережі щоб зробити можливим розгортання на пристроях з обмеженими ресурсами, її побічні ефекти можуть зробити мережу стійкішою до ошукуючих зразків. 
 
 [TODO]
 \cite{defencive-distillation}
 
 Основна ідея методу дистиляції полягає у перенесенні знань з мережі прообразу за допомогою векторів імовірності приналежності зразка до деякого класу, отриманих використовуючи вже існуючу нейронну мережу. На основі цих даних тренується нова нейронна мережа з меншою розмірністю, при чому без значних втрат точності. Ця ідея базується на тому, що знання, здобуті моделлю під час тренування можна отримати не лише безпосередньо з ваг НМ.
 
 TODO Prev paragraph
 
 Як ми уже згадували раніше [TODO], кожен результат передбачення містить у собі деяку інформацію про модель. Таким чином вектори імовірності, отримані з одної моделі містять у собі достатньо інформації, щоб з їх допомогою можна було відтворити модель. Дистиляція "витягує" знання з цих векторів і переносить їх на модель з менш комплексною архітектурою.
 
 Використовуючи уже натреновану та готову до використання модель, кожному зразку тренувальних даних ставиться у відповідність вектор ймовірностей приналежності зразка до кожного з класів, які належать до предметної області моделі. Перевага використання таких прогнозів полягає у тому, що тепер кожна мітка містить не просто жорсткі оцінки, а інформацію про приналежнісь зразка до кожного з класів.
 
 \subsection{Процес тренування та основні параметри} 
 
 \begin{center}
 	\includegraphics[width=10cm]{../images/empty.pdf}
 \end{center}
  \begin{center}
 	\pic{distilation} Схема тренування.
 	
 	[TODO]
 	 Приклад такої мережі зображений у
 	Рисунок 1. Шар softmax - це просто шар, який розглядає a
 	вектор Z (X) виходів, отриманих останнім прихованим шаром
 	DNN, які називаються логітами, і нормалізує їх у
 	вектор ймовірності F (X), вихід DNN, присвоюючи a
 	ймовірність для кожного класу набору даних для введення X.
 	
 \end{center}
 
 Процес дистиляції розпочинається з підготовки мережі-прообразу, вихідний шар якої побудований на основі \textit{нормованої експоненційної функції} -- Softmax.
 
 [TODO] ref Формула
 
 [Maybe not] Процедура тренування прообразу не має строгих обмежені і нічим не відрізняється від звичних підходів. Наприклад, можна застосувати градієнтний спуск на даних з тренувального набору.
 

 [TODO] Означення передавального набору.


 В найпростішій формі дистиляції знання переносяться до дистильованої моделі через тренування на передавальному наборі даних, використовуючи м'які мітки отримані привисоких температурах вихідного шару моделі-оригіналу. Така ж температура використовується під час тренування дистильованої моделі, але після завершення процесу її повертають до стандартного значення, рівного одиниці.
 
 [TODO] Few assumptions

 Проблема цього підходу криється в тому, що точність моделі-прообраза завжди є менша за 100\%. Це означає, що передавальний набір буде містити зразки з неправильними мітками і тренування на ньому нової моделі призведе до втрат точності на тестувальному наборі. Якщо відомі точні класи об'єктів, приклади з неправильними мітками варто виключити з передавального набору або замінити їх на правильні жорсткі мітки. Таким чином процес дистиляції може бути значно покращеним і точність моделі-образу залишиться на високому рівні.
 
 Ще один підхід до вирішення проблем з передавальним набором наводять автори в \cite{distillation}. Під час тренування вони пропонують використовувати середнє арифметичне двох різних функцій оцінки. Перша з них -- функція крос ентропії [TODO] ref, обчислена з використанням такої ж високої температури $T$ в вихідному шарі НМ, яка використовується генерації м'яких міток. Друга функція окінки -- функція крос ентропії з правильними жорсткими мітками при температурі $T=1$.
 
 [TODO] формула
 
 Оскільки велечини градієнтів маштабуються з коефіцієнтом $\frac{1}{T^2}$, то варто домножити їх на $T^2$ при використанні як м'яких, так і жорстких міток. Це гарантує, що відносні часкти вкладу м'якої та жорсткої міток залишатимуться приблизно незмінними, якщо температура буде змінюватися під час експериментів.
 
 [TODO] формула

 
 
 
 
 
 
 У межах
 softmax шар, заданий нейрон, що відповідає індексованому класу
 за $i \in 0 \dots N-1$ (де N - кількість класів) обчислює
 компонент i наступного вихідного вектора F (X):
 
 $$
 F(X)=\left[\frac{e^{z_{i}(X) / T}}{\sum_{l=0}^{N-1} e^{z_{l}(X) / T}}\right]_{i \in 0 . . N-1}
 $$
 
 де $Z(X) = z_0(X), \dots , z_{N-1}$ -- N логітів, що відповідають виходу прихованого шару для кожного з N класів
 у наборі даних, а T - параметр з назвою температура та
 спільного використання через шар softmax. Температура відіграє центральну роль
 роль у основних явищах дистиляції, як ми покажемо пізніше
 у цьому розділі. В контексті дистиляції ми посилаємось на це
 температура як температура дистиляції. Єдине обмеження
 на тренуванні цього першого DNN є те, що висока температура,
 більше 1, слід використовувати в шарі softmax.
 
 Висока температура змушує DNN створювати ймовірність
 вектори з відносно великими значеннями для кожного класу. Дійсно, в
 високі температури, логіти у векторі Z (X) стають незначними
 порівняно з температурою Т. Тому всі компоненти
 вектор імовірності F (X), виражений у рівнянні 2, сходиться до
 1 / N при $T \to \inf$ . Чим вища температура софтмаксу,
 тим більш неоднозначним буде його розподіл ймовірності (тобто всі
 ймовірності виходу F (X) близькі до 1 / N), тоді як
 чим менше температура софтмаксу, тим дискретніше
 його розподіл ймовірностей буде (тобто лише одна ймовірність у
 вихід F (X) близький до 1, а решта близький до 0).
 
 Тоді вектори ймовірності, створені першим DNN, є
 використовується для позначення набору даних. Ці нові мітки називаються м'якими
 етикетки на відміну від ярликів жорсткого класу. Друга мережа з
 тоді менше одиниць навчається за допомогою цього нещодавно позначеного набору даних.
 Як варіант, другу мережу також можна навчити, використовуючи
 поєднання міток жорсткого класу та вектора ймовірностей
 етикетки. Це дозволяє мережі користуватися обома ярликами
 сходитися до оптимального рішення. Знову другий
 мережа тренується при високій температурі softmax, ідентичній
 той, що використовується в першій мережі. Однак ця друга модель
 меншого розміру, досягає порівнянної точності, ніж оригінал
 модель, але є менш обчислювально обчислювальною. Температура
 повертається до 1 під час тестування, щоб отримати більше дискретного
 вектори ймовірності під час класифікації.
 
 
 \subsection{Оцінка захисту}
 
 Фільтрований передавальний набір vs не філтрований
 
 \newpage
 \thispagestyle{empty}
 \section{Методи з гарантованою стійкістю}
 Основна проблема багатьох методів захисту, зокрема і розглянутої раніше захисної дистиляції полягає в тому, що їх ефективність важко, або навіть неможливо довести теоретично. Через це часто виникають випадки, коли захист моделі є ефективним лише для конкретних задач і проти конкретних методів атаки,
 як, наприклад, в [TODO] cite distilation is not robust. Тому цінними є методи, при використанні яких можна гарантувати певний рівень стійкості. Такі методи захисту також називають \textit{сертифікованими}. Один з таких методів ми і роглянемо в даному розділі. Він називається \textit{PixelDP} і був запропонований  запропонований Матіасом Лекуєром та ін. в \cite{pixeldp}. Він базується на принципі \textit{диференційованої конфіденційності (differential privacy)}, в основі якого лежить рандомізація обчислень на великих масивах даних \cite{differential-privacy}.
 
 [TODO]
 \subsection{Захист PixelDP}
 \subsection{Оцінка захисту}
 
 \newpage
 \thispagestyle{empty}
 \section{Комбінація методів та побудова архітектури, стійкої до атак}
 
 \newpage
 \thispagestyle{empty}
 \section{Експерименти?}
 
 \newpage
 \thispagestyle{empty}
 \addcontentsline{toc}{section}{Висновок}
 \section*{Висновок}
 
 \newpage
 \thispagestyle{empty}
 \addcontentsline{toc}{section}{Додатки}
 \section*{Додатки}
 
 \textit{Додаток 1.} Деякі приклади ошукуючих зразків
 
 \textit{Додаток 2.} Таблиці про нм?
 
 \textit{Додаток 3.} Нм для розпізнавання цифр [TODO](prev work)
 
 \textit{Додаток}

 \newpage
 \thispagestyle{empty}
 \addcontentsline{toc}{section}{Література}
 \begin{thebibliography}{99}
	
	\bibitem[1]{first-work}
	\textit{Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus} /
	Intriguing properties of neural networks /
	arXiv preprint arXiv:1312.6199 (2014)
	
	\bibitem[2]{defencive-distillation}
	\textit{Nicolas Papernot, Patrick McDaniel, Xi Wu Somesh Jha,Ananthram Swami} /
	Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks /
	arXiv preprint arXiv:1511.04508 (2016)
	
	\bibitem[3]{distillation}
	\textit{Geoffrey Hinton, Oriol Vinyals, Jeff Dean} /
	Distilling the Knowledge in a Neural Network /
	arXiv preprint arXiv:1503.02531 (2015)
	
	\bibitem[4]{pixeldp}
	\textit{Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana} /
	Certified Robustness to Adversarial Examples with Differential Privacy/
	arXiv preprint arXiv:1802.03471 (2019)
	
	\bibitem[5]{analysis-of-robustness}
	\textit{Alhussein Fawzi, Omar Fawzi, Pascal Frossard} /
	Analysis of classifiers' robustness to adversarial perturbations /
	arXiv preprint arXiv:1502.02590 (2016)
	
	\bibitem[6]{ensemble}
	\textit{Thamas G. Dietterich} /
	Ensemble Methods in Machine Learning /
	Springer (2000)
	
	\bibitem[7]{differential-privacy}
	\textit{Cynthia Dwork} /
	Differential Privacy /
	Automata, Languages and Programming. ICALP (2006)
	
	\bibitem[8]{explaining-a-e}
	\textit{Ian Goodfellow, Jonathon Shlens, Christian Szegedy} /
	Explaining and Harnessing Adversarial Examples /
	arXiv preprint arXiv:1412.6572 (2014)
	
	\bibitem[9]{quarteroni}
	\textit{Alfio Quarteroni, Riccardo Sacco, Fausto Saleri} /
	Numerical Mathematics / --
	Springer, 2000. --300 p.
	
	\bibitem[10]{black-box-2}
	\textit{Nicolas Papernot, Patrick McDaniel, Ian Goodfellow} /
	Transferability in machine learning: from phenomena to black-box attacks using adversarial samples /
	arXiv preprint arXiv:1605.07277 (2016)
	
	\bibitem[11]{zoo}
	\textit{Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh} /
	ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models /
	arXiv preprint arXiv:1708.03999 (2017)
	
	\bibitem[12]{C-and-W}
	\textit{Nicholas Carlini, David Wagner} /
	Towards Evaluating the Robustness of Neural Networks /
	arXiv preprint arXiv:1608.04644 (2017)
	
	\bibitem[13]{my-work}
	\textit{Богдан Бугрій, Юрій Музичук} /
	Атаки на глибокі нейронні мережі /
	(2020)
	
	
 \end{thebibliography}

 \end{document}

